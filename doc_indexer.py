import os
import logging
import faiss
from langchain_community.document_loaders import UnstructuredFileLoader, UnstructuredPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.docstore import InMemoryDocstore
from dotenv import load_dotenv

# ‚úÖ Load environment variables
load_dotenv()

# ‚úÖ Set up logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# ‚úÖ Initialize OpenAI embeddings
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")

# ‚úÖ Define chunking strategy (Larger Chunks with Overlap)
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1200,  # ‚úÖ Increased for better context
    chunk_overlap=300  # ‚úÖ Ensures better continuity between chunks
)

# ‚úÖ Directory containing HR documents
DATA_PATH = "./docs/"
FAISS_INDEX_PATH = "./dbs/docs/faiss_index"

# ‚úÖ Load existing FAISS database (if available)
if os.path.exists(FAISS_INDEX_PATH):
    db = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)
    logging.info("‚úÖ Existing FAISS index loaded.")
else:
    # ‚úÖ Initialize FAISS with an empty index that supports incremental updates
    dimension = len(embeddings.embed_query("test"))  # Get embedding dimension
    faiss_index = faiss.IndexFlatL2(dimension)
    db = FAISS(faiss_index, embeddings, InMemoryDocstore({}), {})
    logging.info("‚ö†Ô∏è No existing FAISS index found. Creating a new one.")

documents = []

# ‚úÖ Scan the docs directory for all supported files
for filename in os.listdir(DATA_PATH):
    file_path = os.path.join(DATA_PATH, filename)

    if file_path.lower().endswith((".pdf", ".docx", ".pptx", ".xlsx")):
        logging.info(f"üìÑ Processing: {file_path}")

        try:
            # ‚úÖ Load file based on format
            if file_path.endswith(".pdf"):
                loader = UnstructuredPDFLoader(file_path, mode="elements", strategy="ocr_only")
            else:
                loader = UnstructuredFileLoader(file_path)

            docs = loader.load()

            # ‚úÖ Apply chunking
            split_docs = text_splitter.split_documents(docs)

            # ‚úÖ Add filename as metadata for reference tracking
            for doc in split_docs:
                doc.metadata["source"] = filename  

            documents.extend(split_docs)

        except Exception as e:
            logging.error(f"‚ùå Error processing {filename}: {e}")

# ‚úÖ Update or Create FAISS index
if documents:
    if db.index.ntotal == 0:  # ‚úÖ If FAISS index is empty, create a new one
        db = FAISS.from_documents(documents, embedding=embeddings)
    else:
        db.add_documents(documents)  # ‚úÖ Properly appends new documents

    db.save_local(FAISS_INDEX_PATH)
    logging.info("‚úÖ Indexing complete. FAISS vector store updated.")
else:
    logging.warning("‚ö†Ô∏è No valid documents found for indexing.")
